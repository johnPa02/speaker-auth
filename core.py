import os
import sys
import queue
import difflib
import numpy as np
import sounddevice as sd
import soundfile as sf
from google.cloud import speech
from dotenv import load_dotenv
import random
from speechbrain.inference.speaker import SpeakerRecognition

load_dotenv()

SAMPLE_SENTENCES = [
    "Xin ch√†o, t√¥i mu·ªën ƒë·∫∑t b√†n cho hai ng∆∞·ªùi v√†o t·ªëi nay l√∫c b·∫£y gi·ªù.",
    "B·∫°n c√≥ th·ªÉ ki·ªÉm tra gi√∫p t√¥i c√≤n b√†n tr·ªëng v√†o cu·ªëi tu·∫ßn n√†y kh√¥ng?",
    "T√¥i mu·ªën ƒë·∫∑t b√†n c·∫°nh c·ª≠a s·ªï ƒë·ªÉ c√≥ kh√¥ng gian y√™n tƒ©nh h∆°n.",
    "Xin vui l√≤ng ghi ch√∫ r·∫±ng t√¥i b·ªã d·ªã ·ª©ng h·∫£i s·∫£n.",
    "Cho t√¥i h·ªèi nh√† h√†ng c√≥ ch·ªó ng·ªìi ngo√†i tr·ªùi kh√¥ng?",
    "T√¥i c·∫ßn m·ªôt b√†n cho b·ªën ng∆∞·ªùi v√†o l√∫c t√°m gi·ªù t·ªëi mai.",
    "L√†m ∆°n x√°c nh·∫≠n ƒë·∫∑t "
    ""
    "ch·ªó c·ªßa t√¥i qua s·ªë ƒëi·ªán tho·∫°i n√†y.",
    "B·∫°n c√≥ th·ªÉ g·ª≠i tin nh·∫Øn x√°c nh·∫≠n qua Zalo ho·∫∑c email kh√¥ng?",
    "T√¥i mu·ªën thay ƒë·ªïi th·ªùi gian ƒë·∫∑t b√†n sang ch√≠n gi·ªù t·ªëi.",
    "Xin gi·ªØ cho t√¥i m·ªôt b√†n ·ªü khu v·ª±c kh√¥ng h√∫t thu·ªëc.",
    "T√¥i c·∫ßn ƒë·∫∑t b√†n cho m∆∞·ªùi ng∆∞·ªùi v√¨ c√≥ ti·ªác sinh nh·∫≠t.",
    "B·∫°n c√≥ th·ªÉ chu·∫©n b·ªã th√™m gh·∫ø em b√© cho ch√∫ng t√¥i kh√¥ng?",
    "T√¥i mu·ªën ƒë·∫∑t ch·ªó ·ªü g·∫ßn s√¢n kh·∫•u ƒë·ªÉ ti·ªán theo d√µi ch∆∞∆°ng tr√¨nh.",
    "Xin vui l√≤ng gi·ªØ b√†n cho t√¥i ƒë·∫øn 7:30",
    "B·∫°n c√≥ th·ªÉ gi·ªõi thi·ªáu m·ªôt s·ªë m√≥n ƒë·∫∑c bi·ªát c·ªßa nh√† h√†ng kh√¥ng?",
    "T√¥i mu·ªën bi·∫øt nh√† h√†ng c√≥ ch·ªó ƒë·∫≠u xe √¥ t√¥ kh√¥ng.",
    "Xin h√£y h·ªßy ƒë·∫∑t ch·ªó c≈© v√† t·∫°o ƒë·∫∑t ch·ªó m·ªõi cho ng√†y mai.",
    "T√¥i c·∫ßn ƒë·∫∑t b√†n ƒÉn tr∆∞a cho nh√≥m s√°u ng∆∞·ªùi v√†o Ch·ªß nh·∫≠t.",
    "B·∫°n c√≥ th·ªÉ cho t√¥i bi·∫øt nh√† h√†ng m·ªü c·ª≠a ƒë·∫øn m·∫•y gi·ªù kh√¥ng?",
    "Xin x√°c nh·∫≠n l·∫°i t√™n v√† s·ªë ƒëi·ªán tho·∫°i c·ªßa t√¥i ƒë·ªÉ ho√†n t·∫•t ƒë·∫∑t ch·ªó."
]

# ====== Load model ======
verification = SpeakerRecognition.from_hparams(
    source="speechbrain/spkrec-ecapa-voxceleb",
    savedir="pretrained_models/spkrec-ecapa-voxceleb"
)

# ================= CONFIG =================
RATE = 16000
CHUNK = int(RATE / 10)  # 100ms
q = queue.Queue()

# ================= GOOGLE STREAMING STT =================
def request_generator():
    while True:
        data = q.get()
        if data is None:
            return
        yield speech.StreamingRecognizeRequest(audio_content=data)

from pydub import AudioSegment, silence

def trim_with_pydub(in_wav, out_wav, silence_thresh=-40, min_silence_len=300):
    """
    Trim silence ·ªü cu·ªëi file b·∫±ng pydub.
    - silence_thresh: ng∆∞·ª°ng dBFS (m·∫∑c ƒë·ªãnh -40 dBFS coi l√† im l·∫∑ng)
    - min_silence_len: bao nhi√™u ms im li√™n t·ª•c th√¨ c·∫Øt
    """
    audio = AudioSegment.from_wav(in_wav)
    nonsilent = silence.detect_nonsilent(
        audio,
        min_silence_len=min_silence_len,
        silence_thresh=silence_thresh
    )
    if nonsilent:
        start, end = nonsilent[0][0], nonsilent[-1][1]
        trimmed = audio[start:end]
    else:
        trimmed = audio
    trimmed.export(out_wav, format="wav")
    print(f"‚úÇÔ∏è Trimmed silence (pydub) + saved: {out_wav}")
    return out_wav


def trim_leading_trailing_silence(audio_np, sr=16000, thresh=500,
                                  min_leading=2000, min_trailing=400):
    abs_audio = np.abs(audio_np)

    # T√¨m ƒëi·ªÉm b·∫Øt ƒë·∫ßu (leading)
    start = 0
    for i in range(len(abs_audio)):
        if abs_audio[i] > thresh:
            start = max(0, i - min_leading)
            break

    # T√¨m ƒëi·ªÉm k·∫øt th√∫c (trailing)
    end = len(abs_audio)
    for i in range(len(abs_audio) - 1, 0, -1):
        if abs_audio[i] > thresh:
            end = min(len(abs_audio), i + min_trailing)
            break

    return audio_np[start:end]


def record_and_transcribe_with_audio(out_wav, language="vi-VN"):
    """Record from mic, send to Google STT, only save audio after speech detected (with pre-buffer), save WAV (trim silence)"""
    client = speech.SpeechClient()
    audio_data = []
    pre_buffer = []
    max_prebuffer_chunks = 10  # ~1s if CHUNK=1600, RATE=16000
    started_speech = False
    def mic_callback(indata, frames, time_, status):
        nonlocal started_speech
        if status:
            print(status, file=sys.stderr)
        data_bytes = bytes(indata)
        q.put(data_bytes)
        data_np = np.frombuffer(data_bytes, dtype=np.int16)
        if started_speech:
            audio_data.append(data_np)
        else:
            pre_buffer.append(data_np)
            if len(pre_buffer) > max_prebuffer_chunks:
                pre_buffer.pop(0)
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=RATE,
        language_code=language,
    )
    streaming_config = speech.StreamingRecognitionConfig(
        config=config,
        interim_results=True,
        single_utterance=False
    )
    transcript = ""
    with sd.RawInputStream(samplerate=RATE, blocksize=CHUNK, dtype="int16",
                           channels=1, callback=mic_callback):
        print("üé§ Please speak, the system will stop automatically when you are silent...")
        audio_requests = request_generator()
        responses = client.streaming_recognize(streaming_config, audio_requests)
        for response in responses:
            if not response.results:
                continue
            result = response.results[0]
            text = result.alternatives[0].transcript.strip()
            if not started_speech and text:
                started_speech = True
                print("üéôÔ∏è Speech detected ‚Üí start saving audio...")
                if pre_buffer:
                    audio_data.extend(pre_buffer)
                    pre_buffer.clear()
            if result.is_final:
                transcript = text
                print(f"‚úÖ Final transcript: {transcript}")
                break
            else:
                print(f"(interim) {text}")
    if audio_data:
        audio_np = np.concatenate(audio_data, axis=0)
        sf.write(out_wav, audio_np, RATE, subtype="PCM_16")
        out_wav = trim_with_pydub(out_wav, out_wav, silence_thresh=-40, min_silence_len=300)
        print(f"‚úÇÔ∏è Trimmed silence + saved: {out_wav}")
    return transcript

def record_and_transcribe_multi(out_wav_prefix, num_utterances=3, language="vi-VN"):
    """
    Record multiple utterances in one session.
    - out_wav_prefix: prefix for saving audio files (e.g., "enrolled/user123_enroll")
    - num_utterances: how many utterances (sentences) to capture
    - language: language code for STT
    Returns:
        transcripts: list of transcripts (strings)
        wav_files: list of saved wav file paths
    """
    transcripts = []
    wav_files = []

    for idx in range(1, num_utterances + 1):
        out_wav = f"{out_wav_prefix}_{idx}.wav"
        print(f"\nüé§ Please speak sentence {idx}/{num_utterances} ...")

        transcript = record_and_transcribe_with_audio(out_wav, language=language)

        if not transcript.strip():
            print("‚ö†Ô∏è No speech detected for this utterance.")
            continue

        transcripts.append(transcript)
        wav_files.append(out_wav)

        print(f"‚úÖ Final transcript {idx}: {transcript}")
        print(f"üíæ Saved: {out_wav}")

    return transcripts, wav_files


# ================= ENROLLMENT =================
def enroll_user_google_streaming(user_id, sample_sentences, enrolled_dir="enrolled", num_enrollments=3):
    os.makedirs(enrolled_dir, exist_ok=True)
    print(f"\nüéØ Starting enrollment for {user_id}")
    enrolled_files = []
    if len(sample_sentences) < num_enrollments:
        raise ValueError("Not enough sample sentences to enroll.")
    selected_sentences = random.sample(sample_sentences, num_enrollments)
    for i, sentence in enumerate(selected_sentences):
        fname_wav = os.path.join(enrolled_dir, f"{user_id}_enroll_{i+1}.wav")
        while True:
            print(f"\nüî¥ Enrollment {i+1}/{num_enrollments}")
            print(f"üìñ Please read the following sentence:\n   \"{sentence}\"")
            transcript = record_and_transcribe_with_audio(fname_wav, language="vi-VN")
            print(f"üìù You just read: \"{transcript}\"")
            ratio = difflib.SequenceMatcher(None, transcript.lower(), sentence.lower()).ratio()
            print(f"üìä Transcript similarity: {ratio:.2f}")
            if ratio >= 0.8:
                print("‚úÖ Valid sentence, moving to the next one.")
                enrolled_files.append(fname_wav)
                break
            else:
                print("‚ùå The sentence was not correct, please try again.")
    enrolled_files_path = os.path.join(enrolled_dir, f"{user_id}_enrolled_files.txt")
    with open(enrolled_files_path, 'w') as f:
        for file_path in enrolled_files:
            f.write(file_path + '\n')
    print(f"\nüéâ Enrollment completed with {num_enrollments} samples for {user_id}")
    return enrolled_files



# ================= VERIFICATION =================
def verify_user(user_id, enrolled_dir="enrolled"):
    """Verify against all enrolled audio files and calculate average score. Uses VAD-based recording and Google STT for transcript."""
    # Load enrolled files list
    enrolled_files_path = os.path.join(enrolled_dir, f"{user_id}_enrolled_files.txt")
    if not os.path.exists(enrolled_files_path):
        print(f"[ERROR] No enrollment found for user: {user_id}")
        return None, False
    with open(enrolled_files_path, 'r') as f:
        enrolled_files = [line.strip() for line in f.readlines() if line.strip()]
    if not enrolled_files:
        print(f"[ERROR] No enrolled audio files found for user: {user_id}")
        return None, False

    # Record verification audio (VAD-based, no fixed duration)
    verify_fname = "verify.wav"
    print(f"\nüîµ Verification - Please speak naturally...")
    transcript = record_and_transcribe_with_audio(verify_fname, language="vi-VN")
    print(f"üìù Transcript: '{transcript}'")

    # Calculate scores against all enrolled files
    scores = []
    predictions = []
    print("\nüìä Comparing with enrolled samples...")
    for i, enrolled_file in enumerate(enrolled_files):
        score, prediction = verification.verify_files(verify_fname, enrolled_file)
        score_val = float(score) if hasattr(score, '__float__') else score.item() if hasattr(score, 'item') else float(score)
        prediction_val = bool(prediction) if isinstance(prediction, (bool, int)) else bool(prediction.item())
        scores.append(score_val)
        predictions.append(prediction_val)
        print(f"Score with enrollment {i + 1}: {score_val:.4f} (prediction: {prediction_val})")

    # Calculate average score and decision
    avg_score = sum(scores) / len(scores)
    final_decision = avg_score > 0.45

    print(f"\n[VERIFY] Average score: {avg_score:.4f}")
    print(f"[VERIFY] Final decision: {final_decision}")

    if final_decision:
        print("‚úÖ Verification successful!")
    else:
        print("‚ùå Verification failed!")

    # Clean up verify audio file
    if os.path.exists(verify_fname):
        os.remove(verify_fname)

    return avg_score, final_decision

# ================= DEMO =================
if __name__ == "__main__":

    # enroll_user_google_streaming("user123", SAMPLE_SENTENCES, num_enrollments=3)
    verify_user("user123")
